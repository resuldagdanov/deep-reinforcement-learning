{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\n",
    "  \"margin-top: 0px;\n",
    "  margin-bottom: 10px;\n",
    "  font-family: sans-serif;\n",
    "  font-size: 8rem;\">\n",
    "<span style=\"color:#808080\">D</span><span style=\"color:#808080\">Q</span><span style=\"color:#808080\">N</span>\n",
    "</h1>\n",
    "\n",
    "In this homework, Jupyter Notebook is mainly used for visualizations and reporting the results. We will start implementing a vanilla DQN agent and continue with implementing a RAINBOW agent. In general, there are 3 scripts to run a training experiment with the DQN agent on an environment. First one is the model where we implement the policy and the loss function. Second one is the Trainer class, where all of the training and evaluation is handled. This class is responsible for parameter updates, running the environment, and keeping track of necessary statistics as well as saving the model (agent and optimizer). Lastly, the third script initiates the agent, trainer, environment, and starts the training with the given arguments.\n",
    "\n",
    "- DQN\n",
    "    - model\n",
    "    - trainer\n",
    "    - box2d (experiment script)\n",
    "\n",
    "We will follow a very similar structure for the Rainbow agent.\n",
    "\n",
    "#### Running\n",
    "\n",
    "We will train each experiment with 5 different seeds to have a good understanding of the stochasticity involved in the training process. You can run your experiments with command-line interface within the notebook.\n",
    "\n",
    "Run the cell below to see CL arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T23:48:33.777923Z",
     "start_time": "2022-04-07T23:48:26.673040Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/dqn/box2d.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example dqn run is given below. (You need to fill the missing parts before running the command below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T23:56:03.715009Z",
     "start_time": "2022-04-07T23:55:26.888181Z"
    },
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/dqn/box2d.py --log_dir logs/vanilla-dqn --gamma 0.9 --n-iterations 40000 --seed 5555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run the training script (box2d.py), the log file named \"progress.csv\" will be saved to the directory given by the ```log_dir``` argument. You can use the csv file to obtain a Pandas dataframe object and visualize the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting\n",
    "\n",
    "When you are done with experiments, you can plot the statistics. We are interested to see how much variation exists in the training. So, run and plot for at least 5 different seeds. Plotter will handle the multi seed plotting and comparisons.\n",
    "\n",
    "Below is an example plot of two experiments each contains 3 different ```progress.csv``` files to demonstrate Plotter.\n",
    "\n",
    "You can switch axes using the dropdowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from dqn.visualize import Plotter\n",
    "\n",
    "\n",
    "def collect_training_logs(log_dir: str) -> Dict[str, List[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "        Obtain pandas frames from progress.csv files in the given directory\n",
    "    \"\"\"\n",
    "    return [pd.read_csv(os.path.join(log_dir, folder, \"progress.csv\"))\n",
    "                        for folder in os.listdir(log_dir)\n",
    "                        if os.path.exists(os.path.join(log_dir, folder, \"progress.csv\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\"gamma-0.90\": collect_training_logs(os.path.join(\"logs\", \"vanilla-dqn-gamma-0.90\")),\n",
    "           \"gamma-0.99\": collect_training_logs(os.path.join(\"logs\", \"vanilla-dqn-gamma-0.99\"))}\n",
    "\n",
    "plotter = Plotter(df_dict)\n",
    "plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "We start filling the source code with ``` dqn/base_dqn.py ```. This class serves as a base class for DQN agents (Vanilla DQN and Rainbow DQN).\n",
    "\n",
    "> Complete ``` greedy_policy ``` in ``` dqn/base_dqn.py ``` script\n",
    "\n",
    "> Complete ``` update_target ``` in ``` dqn/base_dqn.py ``` script\n",
    "\n",
    "\n",
    "> Complete ``` evaluate ``` in ``` dqn/base_dqn.py ``` script\n",
    "\n",
    "As you can see the target network is already initialized in the constructor of the base class. But we also need a replay buffer. The next part to complete is ``` dqn/replaybuffer/uniform.py ```. When we initiate the buffer, we allocate all the memory and then gradually push transitions. Here the capacity is fixed and the size of the buffer grows as we push transitions.\n",
    "\n",
    "> Complete ``` push ``` in ``` dqn/replaybuffer/uniform.py ``` script\n",
    "\n",
    "Remember, our replay buffer is a queue with FIFO behavior.\n",
    "\n",
    "> Complete ``` sample ``` in ``` dqn/replaybuffer/uniform.py ``` script\n",
    "\n",
    "Now we can complete DQN agent.\n",
    "\n",
    "> Complete ``` loss ``` in ``` dqn/dqn/model.py ```\n",
    "\n",
    "When we are done with DQN and replay buffer, we can start implementing trainer class. This class takes care of all the training.\n",
    "\n",
    "> Complete ``` update ``` in ``` dqn/dqn/train.py ```\n",
    "\n",
    "Update function updates the parameters (value and target networks). Also, append td error to the ```td_loss``` list\n",
    "\n",
    "Now we can complete``` __iter__ ``` method. This python special method returns a generator that yields a transition at every step for \"n_iterations\" steps (from ```args```). This is the method where we gather experience from the environment by following ```e_greedy_policy```. To see how we use ```__iter__``` method, please check the ```__call__``` method in the ```Trainer``` class. Additionally, append the episodic training reward if the environment is terminated. Step the epsilon here (comeback this point after next implementation)\n",
    "\n",
    "> Complete ``` __iter__ ``` in ``` dqn/dqn/train.py ```\n",
    "\n",
    "We bring every component necessary for training in the ``` __call__ ``` method. Which is already completed.\n",
    "\n",
    "Before starting the experiments, we need to implement annealing functions located at ``` dqn/common.py ```. Remember epsilon is a python **generator**.\n",
    "\n",
    "> Complete ``` linear_annealing ``` in ``` dqn/common.py ```\n",
    "\n",
    "> Complete ``` exponential_annealing ``` in ``` dqn/common.py ```\n",
    "\n",
    "When trainer is initialized, it selects epsilon annealing based on given ```args```. For example: If the ```epsilon-decay``` is given we use exponential decaying strategy. But if ```epsilon-range``` is given, it we use linear decay.\n",
    "\n",
    "Finally, we need to Q value network\n",
    "\n",
    "> Complete ``` ValueNet ``` in ``` dqn/dqn/box2d.py ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "We run our experiments in the \"Lunar Lander\" environment. Let's see if two of the innovations introduced in the DQN paper make a difference. You can render evaluation episodes using ```--render``` CL argument.\n",
    "\n",
    "> Remember ```n-iterations``` and ```write-period``` must be fixed within each experiment for plotting purposes!\n",
    "In total, there must be 15 runs (5 for each).\n",
    "\n",
    "- Experiment **(run training 5 times)** DQN in Lunar Lander (default) environment with very small Replay Buffer and very frequent target updates (small value for ```target-update-period```)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/dqn/box2d.py --log_dir logs/vanilla-dqn-exp1 --target-update-period 50 --buffer-capacity 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"logs/vanilla-dqn-exp1\"\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "log_dir = os.path.join(\"logs\", \"vanilla-dqn-exp1\")\n",
    "exp_1_dataframes = collect_training_logs(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment DQN in Lunar Lander with large Replay Buffer and target update period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/dqn/box2d.py --log_dir logs/vanilla-dqn-exp2 --target-update-period 300 --buffer-capacity 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"logs/vanilla-dqn-exp2\"\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "log_dir = os.path.join(\"logs\", \"vanilla-dqn-exp2\")\n",
    "exp_2_dataframes = collect_training_logs(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment DQN with exponential decaying epsilon strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/dqn/box2d.py --log_dir logs/vanilla-dqn-exp3 --target-update-period 250 --buffer-capacity 40000 --epsilon-decay 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"logs/vanilla-dqn-exp3\"\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "log_dir = os.path.join(\"logs\", \"vanilla-dqn-exp3\")\n",
    "exp_3_dataframes = collect_training_logs(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining hyperparameters must be tuned and fixed. First two experiments may use linear decaying.\n",
    "\n",
    "Obtain Pandas dataframes (15 in total) and plot the results using the given Plotter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ffb956cbab4d2ead19f3f70479ddda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='X axis', options=('Iteration', 'Episode'), value='Iteration'), Dropdown(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Plotter({\"primitive_dqn\": exp_1_dataframes, \"stable_dqn\": exp_2_dataframes, \"exp_decay_dqn\": exp_3_dataframes})()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train All DQN and Rainbow Experiments at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    !python dqn/dqn/box2d.py --log_dir logs/vanilla-dqn-exp1 --target-update-period 50 --buffer-capacity 10000\n",
    "\n",
    "    !python dqn/dqn/box2d.py --log_dir logs/vanilla-dqn-exp2 --target-update-period 300 --buffer-capacity 50000\n",
    "\n",
    "    !python dqn/dqn/box2d.py --log_dir logs/vanilla-dqn-exp3 --target-update-period 250 --buffer-capacity 40000 --epsilon-decay 0.99\n",
    "\n",
    "    !python dqn/rainbow/box2d.py --n-iterations 5000 --no-double --no-dueling --no-noisy --no-prioritized --n-steps 1 --no-dist\n",
    "\n",
    "    !python dqn/rainbow/box2d.py --log_dir logs/prioritized --no-dist --no-dueling --n-step 1 --no-double --no-noisy\n",
    "\n",
    "    !python dqn/rainbow/box2d.py --log_dir logs/distributional --no-prioritized --no-dueling --n-step 1 --no-double --no-noisy\n",
    "\n",
    "    !python dqn/rainbow/box2d.py --log_dir logs/nsteps --no-prioritized --no-dist --no-dueling --n-step 5 --no-double --no-noisy\n",
    "\n",
    "    !python dqn/rainbow/box2d.py --log_dir logs/double --no-prioritized --no-dist --no-dueling --n-step 1 --no-noisy\n",
    "\n",
    "    !python dqn/rainbow/box2d.py --log_dir logs/dueling --no-prioritized --no-dist --n-step 5 --no-double --no-noisy\n",
    "\n",
    "    !python dqn/rainbow/box2d.py --log_dir logs/noisy --no-prioritized --no-dist --no-dueling --n-step 1 --no-double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"margin-top: 0px;\n",
    "  margin-bottom: 10px;\n",
    "  font-family: sans-serif;\n",
    "  font-size: 8rem;\">\n",
    "<span style=\"color:#FF0000\">R</span><span style=\"color:#FFDB00\">a</span><span style=\"color:#49FF00\">i</span><span style=\"color:#00FF92\">n</span><span style=\"color:#0092FF\">b</span><span style=\"color:#4900FF\">o</span><span style=\"color:#FF00DB\">w</span>\n",
    "</h1>\n",
    "\n",
    "We use DQN as a base class for our implementation. Rainbow introduces a few extensions over vanilla DQN. Each of these extensions can be disabled in our implementation. We will test the Rainbow agent in both Lunar Lander and Pong.\n",
    "\n",
    "> **Read** the related paper or the book section before moving to implementation.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Before implementing extensions we need to have a bare minimum DQN so that you can test your extension independently. We feed ```extensions``` dictionary to Rainbow agent. The dictionary contains information related to extensions that we want to use in RAINBOW agent. You can see the definition of ```extensions``` dictionary in ```dqn/rainbow/box2d.py```.\n",
    "\n",
    "Luckly, we already have a \"vanilla\" DQN to start with. We only need to complete a few parts to run vanilla DQN (one that has no extension) in rainbow agent. \n",
    "\n",
    "> Complete ```ValueNet``` in ```dqn/rainbow/box2d.py```. Ignore the ```extensions``` dictionary for now.\n",
    "\n",
    "Most of the methods use inherited functions from DQN section. However, as you implement the extensions you will need to replace them with their extension based versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/rainbow/box2d.py --n-iterations 5000 --no-double --no-dueling --no-noisy --no-prioritized --n-steps 1 --no-dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Prioritized Buffer\n",
    "Let's start with Prioritized Replay Buffer. To start implementing this buffer we need weighted sampling. NumPy has ```np.random.choice``` function that we can use for prioritized sampling. \n",
    "\n",
    "\n",
    "> Complete ``` PriorityBuffer ``` in ``` dqn/replaybuffer/prioritized.py ```.\n",
    "> - ```push```\n",
    "> - ```sample```\n",
    "> - ```update_priority```\n",
    "\n",
    "**Prioritized Buffer** causes a few changes in the code.\n",
    "- ```update``` function in the ```Trainer``` class located at ```dqn/rainbow/train.py```\n",
    "- ``` loss ``` functions (two of them) in the ```Rainbow``` class. Loss tensor must not be averaged over the batch axis! In the ```update``` function we will be using the weighted average loss where the weights are Importance sampling weights obtained from  Prioritized Buffer sample (see the paper for further details). Also, update the td errors of the samples. \n",
    "\n",
    "> Modify ```update``` method in  ```dqn/rainbow/train.py```.\n",
    "\n",
    "> Modify ```vanilla_loss``` in ```dqn/rainbow/model.py```.\n",
    "\n",
    "> Modify ```_next_action_network``` in ```dqn/rainbow/model.py```. (We will comeback to this one in Double & Noisy extensions)\n",
    "\n",
    "Use ```_next_action_network``` to obtain target actions so that the loss functions become compatible with double Q-learning.\n",
    "\n",
    "Remember this while implementing ```update``` function!\n",
    "\n",
    "You can run Prioritized Buffer experiments bash script under the experiments section to test your implementation.\n",
    "\n",
    "- - -\n",
    "\n",
    "#### Distributional RL\n",
    "\n",
    "This extension changes Q value and hence loss function and policy need modifications. Greedy policy need the expected value of Q distribution, therefore we need to implement additional method that we can use in greedy policy. Moreover, we need to have a Q network with more outputs ```(act_size * n_atoms)``` instaed of ```act_size```\n",
    "\n",
    "> Complete ```distributional_loss ``` in ```dqn/rainbow/model.py```.\n",
    "\n",
    "> Complete ```expected_value ``` in ```dqn/rainbow/model.py```.\n",
    "\n",
    "> Modify ```HeadLayer ``` in ```dqn/rainbow/layers.py```.\n",
    "- - -\n",
    "\n",
    "#### N-step Learning\n",
    "\n",
    "There are many ways of using n-step learning, so we will pick the simplest one. Ignore Importance sampling ratios. Yield a transition with a reward that equals to the sum of $n$ consecutive rewards (discounted by gamma) and the nth next state as the next_state. You can find this way of using n-step learning in Chapter 7 of the textbook (without Importance Sampling or Tree Backup, similar to n-step Sarsa). You can use ```deque```s to delay yielding transitions.\n",
    "\n",
    "$(s_t, a_t, \\sum_{j=t}^{t+n}(\\gamma^{j-t} r_t), \\text{done}, s_{t+n})$\n",
    "\n",
    "> Complete ``` __iter__``` in ``` dqn/rainbow/train.py ```\n",
    "\n",
    "We set n to 1 to deactivate this extension.\n",
    "\n",
    "- - -\n",
    "#### Double Q-learning\n",
    "\n",
    "In double Q learning, the target value is calculated using the actions selected from the online network(```valuenet```). Since we already use ```_next_action_network``` function to find the action that yields maximum value at the next state, we only need to implement ```_next_action_network``` method in ```dqn/rainbow/model.py```.\n",
    "\n",
    "> Modify ```_next_action_network``` in ```dqn/rainbow/model.py```.\n",
    "\n",
    "- - -\n",
    "#### Noisy Net\n",
    "\n",
    "In this part, we need to complete ```NoisyLayer``` at ```dqn/rainbow/layers.py```. Moreover, when we use \"noisy-network\" we can act greedily since the stochasticity is built within the network. In ```__iter__``` method at ``` dqn/rainbow/train.py``` use ```greedy_policy``` if noisy-net is active.\n",
    "\n",
    "> Complete ```NoisyLinear``` in ```dqn/rainbow/layers.py```.\n",
    "> - __init__\n",
    "> - reset_noise\n",
    "> - forward\n",
    "\n",
    "> Modify ```update``` in  ```dqn/rainbow/train.py``` to reset noise if noisy network is active. Reset both the target and the online networks separately.\n",
    "\n",
    "> Modify ```__iter__``` in  ```dqn/rainbow/train.py``` to use greedy (but noisy) policy for exploration.\n",
    "\n",
    "> Modify ```ValueNet``` in ```dqn/rainbow/box2d.py```.\n",
    "\n",
    "> Modify ```ValueNet``` in ```dqn/rainbow/pong.py``` when you start working with Pong.\n",
    "\n",
    "> Modify ```HeadLayer``` in ```dqn/rainbow/layers.py```.\n",
    "\n",
    "In eval mode, use parameter means. Do not forget to use eval mode for target value calculations.\n",
    "\n",
    "\n",
    "- - - \n",
    "#### Dueling Architecutre\n",
    "\n",
    "You can implement Dueling architecture by filling the ```HeadLayer``` class at ```dqn/rainbow/layers.py```. Remember, the structure of this class depends on Dueling, Distributional, and Noisy Nets.\n",
    "\n",
    "> Modify ```HeadLayer``` in ```dqn/rainbow/layers.py```.\n",
    "\n",
    "- - - \n",
    "#### Rainbow\n",
    "\n",
    "Once you completed all the extensions you can combine them. Complete the implementation by filling:\n",
    "\n",
    "- In box2D, initialize a fully connected network.\n",
    "> Complete ```ValueNet``` in ```dqn/rainbow/box2d.py``` use ```HeadLayer``` and ```NoisyLinear``` layers if noisy is activated\n",
    "- In pong, initialize a convolutional network that reduces the spatial size into 5 by 5 (or any other value that you prefer). \n",
    "> Complete ```ValueNet``` in ```dqn/rainbow/pong.py``` use ```HeadLayer``` and ```NoisyLinear``` layers if noisy is activated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "We will test each extension on its own. Run ```box2d.py``` by enabling one extension at a time and store the results (5 runs per experiment). An example run is given below for prioritized-only experiment.\n",
    "\n",
    "> Remember ```n-iterations``` and ```write-period``` must be fixed within each experiment for plotting purposes!\n",
    "In total, there must be 30 runs (5 for each)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN with Prioritized Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/rainbow/box2d.py --log_dir logs/prioritized --no-dist --no-dueling --n-step 1 --no-double --no-noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN with Distributional Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/rainbow/box2d.py --log_dir logs/distributional --no-prioritized --no-dueling --n-step 1 --no-double --no-noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN with N-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/rainbow/box2d.py --log_dir logs/nsteps --no-prioritized --no-dist --no-dueling --n-step 5 --no-double --no-noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN with Double Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/rainbow/box2d.py --log_dir logs/double --no-prioritized --no-dist --no-dueling --n-step 1 --no-noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN with Dueling Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/rainbow/box2d.py --log_dir logs/dueling --no-prioritized --no-dist --n-step 5 --no-double --no-noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN with Noisy Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/rainbow/box2d.py --log_dir logs/noisy --no-prioritized --no-dist --no-dueling --n-step 1 --no-double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_vanilla = collect_training_logs(os.path.join(\"logs\", \"vanilla-dqn-exp2\"))\n",
    "dataframes_prioritized = collect_training_logs(os.path.join(\"logs\", \"prioritized\"))\n",
    "dataframes_distributional = collect_training_logs(os.path.join(\"logs\", \"distributional\"))\n",
    "dataframes_nsteps = collect_training_logs(os.path.join(\"logs\", \"nsteps\"))\n",
    "dataframes_double = collect_training_logs(os.path.join(\"logs\", \"double\"))\n",
    "dataframes_dueling = collect_training_logs(os.path.join(\"logs\", \"dueling\"))\n",
    "dataframes_noisy = collect_training_logs(os.path.join(\"logs\", \"noisy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results using the provided Plotter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotter(\n",
    "    {\"dqn\": dataframes_vanilla,\n",
    "     \"dqn+prioritized_buffer\": dataframes_prioritized,\n",
    "     \"dqn+distributional\": dataframes_distributional,\n",
    "     \"dqn+n_step\": dataframes_nsteps,\n",
    "     \"dqn+double_q\": dataframes_double,\n",
    "     \"dqn+dueling\": dataframes_dueling,\n",
    "     \"dqn+noisy_nets\": dataframes_noisy,\n",
    "    }\n",
    ")()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can remove the ones that you did not implement from the plots.\n",
    "\n",
    "> Feel free to experiment with hyperparameters. You can plot their scores and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATARI**\n",
    "\n",
    "The next step is to train **Pong** with the Rainbow agent. This time, please enable model saving ```--save-model``` and upload the model parameters that returns the highest evaluation score to google drive. Put the link at the end of the notebook. You can use any combination of extensions.\n",
    "\n",
    "> **Note**: No need to run Pong for more than 1 run!\n",
    "\n",
    "> **Note**: You need GPU for this experiment! You can use [Colab](https://colab.research.google.com/) if you do not have access to a GPU machine.\n",
    "\n",
    "Before starting a long training make sure that pong.py terminates successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python dqn/rainbow/pong.py --log_dir logs/pong --no-prioritized --no-dist --no-dueling --n-step 1 --no-noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "Plotter({\"dqn\": collect_training_logs(os.path.join(\"logs\", \"pong\"))})()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put Google drive [link](?) for the model paramterers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "8048370e5f4472fc8019254cb1a7d72a501871a1608696a966bb718ea10e97a3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
