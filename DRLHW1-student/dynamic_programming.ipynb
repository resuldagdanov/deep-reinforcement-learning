{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DP\n",
    "\n",
    "Dynamic programming exploits the Markov property to find an optimal policy. The main idea in Dynamic Programming (DP) is to use memory to store values that we calculated instead of recalculating them over and over. DP methods are guaranteed to find optimal policy and values. However, these methods require full knowledge of the MDP.\n",
    "\n",
    "Recall the MazeWorld environment from the MDP notebook. It is fully deterministic. But this is not the case for most of the problems. In this notebook, we will be working on StochasticMaze. Which has following properties:\n",
    "\n",
    "- The agent does not move if the chosen action leads to a non-passable state.\n",
    "- Transitions in the environments are stochastic.\n",
    "- Intended action is chosen 70% of the time and one of the remaining 3 actions is chosen with 10% probability\n",
    "- The environment terminates when the state with the goal is reached.\n",
    "- Rewards obtained during a transition depends only on the next state.\n",
    "\n",
    "In order to use DP methods, we need to build a map of transitions. This map has a structure as given below:\n",
    "\n",
    "\n",
    "```\n",
    "transition_map[state][action] -> [(probability, neighboor_state, reward, termination), ...]\n",
    "\n",
    "```\n",
    "\n",
    "**Note** that, each state-action pair points to a list of possible transitions and each transition contains 4 tuples of probability, next state, reward, and termination (binary) values. Since the agent can only transition into 4 different states at max, the length of the list is at most 4 for each state-action pair.\n",
    "\n",
    "Let's render the initial board of the environemnt. \n",
    "<span style=\"color:#989898\">Dark gray cells</span> are impassable while\n",
    "<span style=\"color:#DADADA\">light gray cells</span> are passable empty cells.\n",
    "<span style=\"color:#00B8FA\">Blue cell</span> represents the agent and\n",
    "<span style=\"color:#DADA22\"> golden cell</span> reprensents the goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T23:15:06.046158Z",
     "start_time": "2021-04-13T23:15:05.969220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "CELL_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55f1545fcd44a8daf3d4b550692e6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Canvas(layout=Layout(height='500px', width='700px'), size=(700, 500)),), layout=Layout(height='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rl_hw1.env.mazeworld import StochasticMaze\n",
    "\n",
    "env = StochasticMaze(cell_size=CELL_SIZE)\n",
    "state = env.reset()\n",
    "board = env.board.copy()\n",
    "state\n",
    "\n",
    "env.step(1)\n",
    "\n",
    "# in order to visualize this must be the last line of the cell\n",
    "env.init_render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the board of the game using ```env.board```. Note that the game must be initiated at least once.\n",
    "\n",
    "**Question 1)** In the ```rl_hw1/dp/transition.py``` module, implement ```make_transition_map``` as described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stated that values in a Markov chain can be evaluated iteratively. This procedure is called **Policy Evaluation**. In order to update the policy, we need to know the values at each state so that we can improve it. Since we know everything about the MDP, we can iteratively compute the optimal value for a fixed policy.\n",
    "\n",
    "Policy evaluation at a state $s$ can be simply calculated by following the equation shown below:\n",
    "$$ V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$$ \n",
    "$$ V^\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s]$$ \n",
    "$$ V^\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma V^\\pi(S_{t+1}) | S_t = s]$$\n",
    "$$ V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r|s, a)\\big[r + \\gamma V^\\pi(s')\\big]$$\n",
    "\n",
    "Uppercase letters denote random variables while lowercase letters denote scalar values.\n",
    "\n",
    "**Question 2)** In the third equation, $ G_{t+1} $ replaced with $V^\\pi(S_{t+1})$. Why not it is replaced with $V^\\pi(s')$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $s'$ is only obtained after calculating a given set of probabilities $p(s', r|s, a)$ where $a$ is an action at a current state $s$ resulting in a reward $r$. $V^\\pi(S_{t+1})$ could be calculated by taking the expectation of applying policy $\\pi$. In order to compute this expectation, summation of probabilities $p(s', r|s, a)$ of all possible states $s \\in S$ given an action $a$. The next state is dependent on an expectation, so it has to be defined in a random variable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3)** In the ```rl_hw1/dp/methods.py``` module, implement ```one_step_policy_eval``` in the ```DPAgent``` class.\n",
    "\n",
    "Let's test your implementation to see if we can calculate the values. We will be visualizing the values that will be drawn on the right side of the canvas. Remember that the initial policy is random so, we expect to see some sort of diffusion. Let's iterate the ```one_step_policy_eval``` 40 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4)** To test your policy evaluation code we need a policy. So, in the ```rl_hw1/dp/methods.py``` module, implement ```policy``` in the ```DPAgent``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2069d600e88245c3a12e43bc15412106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Canvas(layout=Layout(height='500px', width='700px'), size=(700, 500)),), layout=Layout(height='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = StochasticMaze(cell_size=CELL_SIZE)\n",
    "\n",
    "# in order to visualize this must be the last line of the cell\n",
    "env.init_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_hw1.dp import make_transition_map, DPAgent\n",
    "\n",
    "tmap = make_transition_map(board)\n",
    "agent = DPAgent(4, tmap)\n",
    "\n",
    "# color map for values\n",
    "cmap = cm.get_cmap(\"viridis\", 100)\n",
    "\n",
    "# initiating a 2D value image\n",
    "values = np.zeros_like(board)\n",
    "\n",
    "for i in range(40):\n",
    "    agent.one_step_policy_eval(gamma=0.95)\n",
    "    \n",
    "    # filling the value image\n",
    "    for key, val in agent.values.items():\n",
    "        values[key] = val * 100\n",
    "    \n",
    "    # painting new values\n",
    "    env._renderer.draw(values, 0, CELL_SIZE * (board.shape[1] + 1), cmap)\n",
    "    time.sleep(1 / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate updated values. The next step is to improve the policy according to the updated values. We call this procedure **policy improvement**. At each state, we update the policy by changing action probabilities so that the policy is improved.\n",
    "\n",
    "**Question 5)** in the ```rl_hw1/dp/methods.py``` module, implement ```policy_improvement``` in the ```DPAgent``` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6)**  Why do we need to evaluate values whenever we improve the policy? Is it possible to find the optimal values(values of the optimal policy) without using policy improvement(explain your answer)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything necessary to move on to Policy and Value iteration methods. The idea behind them is very simple. But before defining them, let's look at what we have. On one hand, we can evaluate the current value iteratively; on the other hand, we can instantly improve our policy to make it follow the high-value path. The first approach that comes into mind is to call these methods in turns. We can find the perfect values by calling ```policy_evaluation``` until convergence and improve the policy with ```policy_improvement```. This method is called **Policy Iteration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7)** in the ```rl_hw1/dp/methods.py``` module, implement ```PolicyIteration``` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One drawback of the **Policy Iteration** is that we need to evaluate values until convergence after each ```policy_improvement``` call. It is observed that we don't need perfect values to improve the policy. Instead of evaluating the values until convergence we can call ```policy_improvement``` after each value evaluation step. This strategy is called **Value Iteration**.  \n",
    "\n",
    "**Question 8)** in the ```rl_hw1/dp/methods.py``` module, implement ```ValueIteration``` class.\n",
    "\n",
    "Now, let's compare these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(agent, env, x_offset, y_offset):\n",
    "    # Scroll up before it starts\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Initiating a 2D value image\n",
    "    values = np.zeros_like(board)\n",
    "    time.sleep(1.0)\n",
    "    for i in range(20):\n",
    "        agent.optimize(0.95)\n",
    "        # Filling the value image\n",
    "        for key, val in agent.values.items():\n",
    "            values[key] = val * 100\n",
    "        # Painting new values\n",
    "        env._renderer.draw(values, x_offset, y_offset, cmap)\n",
    "        time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_loop(agent, env):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.policy(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        time.sleep(1/10)\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_hw1.dp import make_transition_map, PolicyIteration, ValueIteration\n",
    "\n",
    "CELL_SIZE = 21\n",
    "tmap = make_transition_map(board)\n",
    "pi_agent = PolicyIteration(tmap)\n",
    "vi_agent = ValueIteration(tmap)\n",
    "# Color map for values\n",
    "cmap = cm.get_cmap(\"viridis\", 100)\n",
    "env = StochasticMaze(cell_size=CELL_SIZE)\n",
    "env.init_render() # In order to visualize this must be the last line of the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteartion\n",
    "training_loop(pi_agent, env, 0, CELL_SIZE*(board.shape[1]+1))\n",
    "running_loop(pi_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteartion\n",
    "training_loop(vi_agent, env, 0, CELL_SIZE*(board.shape[1]*2+2))\n",
    "running_loop(vi_agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see both having reasonable value diffusions and near-optaimal policies(can reach the goal) then **well done!**"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
