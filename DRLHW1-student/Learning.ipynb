{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD METHODS\n",
    "\n",
    "In DP methods, we found optimal policies. But, we used ```transition_map``` to do that. This is quite impractical. There are many reasons for that:\n",
    "\n",
    "- We don't know all the states we can visit\n",
    "- Environment dynamics can be unknown\n",
    "- Iterating over all states can be computationally infeasible\n",
    "\n",
    "In this notebook, we will be practicing over TD methods. Instead of calculating the exact value, we will get samples and estimate the value over samples. One approach that comes into our minds, when we talk about sampling, is **Monte Carlo** method. MC method is the first algorithm we will be implementing.\n",
    "\n",
    "**MC** estimation and control is pretty simple. We sample trajectories from the environment by following a policy $\\pi$ and calculate the returns of each state we visit.\n",
    "\n",
    "**Question 1)** How can you ensure that we visit (sample) enough states to find an optimal policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not yet formally proven (Tsitsiklis, 2002) how many samples are required or which samples has to be visited to find an optimal policy. Some techniques are developed over the year to solve this issue by exploration-exploitation dillemma. In example, with epsilon-greedy algorithm, as training time passes, the exploration amount decreases and exploitation increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "\n",
    "We will be using a new environment called **Warehouse** where an agent tries to match each item with the corresponding box. Building a transition map for this environment would be painful. Luckily, we don't need to build that in TD methods. The ```worldmap``` is given below (you can modify this for fun but, we will be using the one below to evaluate).\n",
    "\n",
    "You can run the cell below to visualize the environment.\n",
    "- <span style=\"color:#989898\">Dark gray cells</span> are impassable cells.\n",
    "- <span style=\"color:#DADADA\">light gray cells</span> are passable empty cells.\n",
    "- <span style=\"color:#00B8FA\">Blue cell</span> is the agent.\n",
    "- <span style=\"color:#A33675\"> Darker magenta cell </span> and <span style=\"color:#48C69F\"> darker green cell </span> are items to collect. \n",
    "- <span style=\"color:#B34685\"> Lighter magenta cell </span> and <span style=\"color:#58D6AF\"> lighter green cell </span> are the boxes.\n",
    "\n",
    "Pairing is also given below. A key in the pairing dictionary is a box(uppercase) and its corresponding value is a list of items(lowercase) that can be delivered to that box.\n",
    "\n",
    "State representation is different here. Instead of giving just the position of the agent, we have 4 more additional features. These are 4 boolean values for two items and two boxes.  The boolean feature is true if the item or the box exists on the map and false otherwise. When the bucket receives the item it disappears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T23:32:50.829617Z",
     "start_time": "2021-04-13T23:32:50.630806Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d2d1e61f6b49a398fa814c2770a459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Canvas(layout=Layout(height='500px', width='700px'), size=(700, 500)),), layout=Layout(height='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rl_hw1.env import Warehouse\n",
    "\n",
    "worldmap = [\"#########\",\n",
    "            \"#   #   #\",\n",
    "            \"# c # C #\",\n",
    "            \"#   #   #\",\n",
    "            \"#   P   #\",\n",
    "            \"#   #   #\",\n",
    "            \"# b # B #\",\n",
    "            \"#   #   #\",\n",
    "            \"#########\"]\n",
    "\n",
    "# buckets are Uppercase letters while balls are lowercase\n",
    "# matching is done so that the ball \"b\" must be carried to the bucket \"B\"\n",
    "pairing = {\n",
    "    \"B\": [\"b\"],\n",
    "    \"C\": [\"c\"]\n",
    "}\n",
    "\n",
    "env = Warehouse(balls=\"cb\", buckets=\"BC\", pairing=pairing, worldmap=worldmap)\n",
    "env.init_render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2)** In the ```rl_hw1/learning/monte_carlo.py``` module, implement ```MonteCarloAgent```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the ```MonteCarloAgent``` using the training loop given below. You can tune hyperparameters (```args```) given below. ```args``` is a namedtuple built from the dictionary initialized between lines 10 and 18. \n",
    "\n",
    "**Question 3)** Train the ```MonteCarloAgent``` by tuning the hyperparameters. For all of your implementations (MC, Q learning, Sarsa) your agent should get at least **1.5** episodic reward (sum of rewards throughout an episode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, reward: 0.0\n",
      "Episode: 100, reward: 0.02\n",
      "Episode: 150, reward: 0.04\n",
      "Episode: 200, reward: 0.02\n",
      "Episode: 250, reward: 0.0\n",
      "Episode: 300, reward: 0.0\n",
      "Episode: 350, reward: 0.0\n",
      "Episode: 400, reward: 0.0\n",
      "Episode: 450, reward: 0.02\n",
      "Episode: 500, reward: 0.03\n",
      "Episode: 550, reward: 0.01\n",
      "Episode: 600, reward: 0.0\n",
      "Episode: 650, reward: 0.0\n",
      "Episode: 700, reward: 0.0\n",
      "Episode: 750, reward: 0.0\n",
      "Episode: 800, reward: 0.0\n",
      "Episode: 850, reward: 0.0\n",
      "Episode: 900, reward: 0.0\n",
      "Episode: 950, reward: 0.0\n",
      "Episode: 1000, reward: 0.0\n",
      "Episode: 1050, reward: 0.0\n",
      "Episode: 1100, reward: 0.0\n",
      "Episode: 1150, reward: 0.0\n",
      "Episode: 1200, reward: 0.0\n",
      "Episode: 1250, reward: 0.0\n",
      "Episode: 1300, reward: 0.0\n",
      "Episode: 1350, reward: 0.0\n",
      "Episode: 1400, reward: 0.0\n",
      "Episode: 1450, reward: 0.0\n",
      "Episode: 1500, reward: 0.0\n",
      "Episode: 1550, reward: 0.0\n",
      "Episode: 1600, reward: 0.0\n",
      "Episode: 1650, reward: 0.0\n",
      "Episode: 1700, reward: 0.0\n",
      "Episode: 1750, reward: 0.0\n",
      "Episode: 1800, reward: 0.0\n",
      "Episode: 1850, reward: 0.0\n",
      "Episode: 1900, reward: 0.0\n",
      "Episode: 1950, reward: 0.0\n",
      "Episode: 2000, reward: 0.0\n",
      "Episode: 2050, reward: 0.0\n",
      "Episode: 2100, reward: 0.0\n",
      "Episode: 2150, reward: 0.0\n",
      "Episode: 2200, reward: 0.0\n",
      "Episode: 2250, reward: 0.0\n",
      "Episode: 2300, reward: 0.0\n",
      "Episode: 2350, reward: 0.0\n",
      "Episode: 2400, reward: 0.0\n",
      "Episode: 2450, reward: 0.0\n",
      "Episode: 2500, reward: 0.0\n",
      "Episode: 2550, reward: 0.0\n",
      "Episode: 2600, reward: 0.0\n",
      "Episode: 2650, reward: 0.0\n",
      "Episode: 2700, reward: 0.0\n",
      "Episode: 2750, reward: 0.0\n",
      "Episode: 2800, reward: 0.0\n",
      "Episode: 2850, reward: 0.0\n",
      "Episode: 2900, reward: 0.0\n",
      "Episode: 2950, reward: 0.0\n",
      "Episode: 3000, reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "from rl_hw1.learning import MonteCarloAgent\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# initialize agent\n",
    "agent = MonteCarloAgent(nact=4)\n",
    "\n",
    "# hyperparameters\n",
    "args = dict(\n",
    "    iteration = 3000,\n",
    "    gamma = 0.98,\n",
    "    alpha = 0.03,\n",
    "    init_eps = 0.5,\n",
    "    final_eps = 0.1,\n",
    "    eps_decay_rate = 0.999,\n",
    "    seed = 12021,            # current year: (10000 + 2021 = 12021) Holocene calendar\n",
    ")\n",
    "args = namedtuple('args', args.keys())(*args.values()) \n",
    "\n",
    "# seed\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "# traning loop\n",
    "reward_list = []\n",
    "epsilon = args.init_eps\n",
    "\n",
    "for ix in range(args.iteration):\n",
    "    epsilon = max(args.final_eps, epsilon*args.eps_decay_rate)\n",
    "    \n",
    "    reward = agent.one_episode_train(env, lambda x: agent.e_greedy_policy(x, epsilon), args.gamma, args.alpha)\n",
    "    reward_list.append(reward)\n",
    "    \n",
    "    if ((ix + 1) % 50) == 0:\n",
    "        print(\"Episode: {}, reward: {}\".format(ix + 1, np.mean(reward_list[-100:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to render the trained agent. Run the followwing cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe415b446524cf28380236f99a57203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Canvas(layout=Layout(height='500px', width='700px'), size=(700, 500)),), layout=Layout(height='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = Warehouse(balls=\"cb\", buckets=\"BC\", pairing=pairing, worldmap=worldmap)\n",
    "env.init_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can run this cell to visualize the agent as many times as you wish.\n",
    "agent.evaluate(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC method waits until the episode is terminated to begin the update. But, is it possible to update the policy within every transition? Temporal Difference (TD) methods exactly aim for this. There are two popular TD methods that you will be implementing: Q Learning and SARSA.\n",
    "\n",
    "**Question 3)** What is the difference between off-policy and on-policy algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Off-policy algorithms do evaluation and update of the policy which is different from the policy that is used to generate episode transitions. On the other hand, on-policy algorithms do evaluation and update of the policy that is used to generate episode transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Q - Learning\n",
    "\n",
    "Q learning is an off-policy algorithm that employs temporal difference value estimation.\n",
    "\n",
    "**Question 4)** What makes Q learning an off-policy algorithm? Why don't we use importance sampling in Q learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the learned value function is defined under the __target policy__ that is used to generate data, a Q learning method automatically becomes an off-policy algorithm. In order to transform behaviors of expectations from learning policy to the target policy, by taking the ratio of action probabilities, __importance sampling__ techniques are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5)**  In the ```rl_hw1/learning/td.py``` module, implement ```QAgent```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rl_hw1.learning import QAgent, SarsaAgent\n",
    "\n",
    "\n",
    "# Initiate environment\n",
    "env = Warehouse(balls=\"cb\", buckets=\"BC\",\n",
    "                pairing=pairing, worldmap=worldmap)\n",
    "\n",
    "# Initialize agent\n",
    "q_agent = QAgent(nact=4)\n",
    "# Hyperparameters\n",
    "args = dict(\n",
    "    episodes = 1500,\n",
    "    evaluate_period = 50,\n",
    "    gamma = 0.98,\n",
    "    alpha = 0.03,\n",
    "    init_eps = 0.9,\n",
    "    final_eps = 0.05,\n",
    "    eps_decay_rate = 0.9995,\n",
    "    seed = 12021,            # Current year: (10000 + 2021 = 12021) Holocene calendar\n",
    ")\n",
    "args = namedtuple('args', args.keys())(*args.values()) \n",
    "\n",
    "# Seed\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "\n",
    "q_rewards = q_agent.train(env, q_agent.e_greedy_policy, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np.convolve(q_rewards, np.ones(10)/10, \"valid\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.init_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run this cell to visualize the agent as many times as you wish.\n",
    "q_agent.evaluate(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You implemented an off-policy TD(Temporal Difference) Learning algorithm. Now, try to implement SARSA. You can find the pseudocode in the 6th chapter of Reinforcement Learning Book (Sutton & Barta).\n",
    "\n",
    "**Question 6)** What makes SARSA an on-policy algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7)** In the ```rl_hw1/learning/td.py``` module, implement ```SarsaAgent```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8)** Write down a training loop similar to what we did for Q learning to test your ```SarsaAgent```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa training loop\n",
    "#  ______   _____   _        _\n",
    "# |  ____| |_   _| | |      | |\n",
    "# | |__      | |   | |      | |\n",
    "# |  __|     | |   | |      | |\n",
    "# | |       _| |_  | |____  | |____\n",
    "# |_|      |_____| |______| |______|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa reward visualization\n",
    "#  ______   _____   _        _\n",
    "# |  ____| |_   _| | |      | |\n",
    "# | |__      | |   | |      | |\n",
    "# |  __|     | |   | |      | |\n",
    "# | |       _| |_  | |____  | |____\n",
    "# |_|      |_____| |______| |______|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa visualization\n",
    "#  ______   _____   _        _\n",
    "# |  ____| |_   _| | |      | |\n",
    "# | |__      | |   | |      | |\n",
    "# |  __|     | |   | |      | |\n",
    "# | |       _| |_  | |____  | |____\n",
    "# |_|      |_____| |______| |______|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9)** Compare MC and TD based methods. When would you prefer one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10)** Can you think of an algorithm that combines the advanteges of MC and TD methods. Write down a **simple**  pseudocode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen different ways to estimate value, however, until now we only worked on tabular representations. But almost always states are represented by vectors. In this part of the homework, you will be working on linear representations that can work with continuous state spaces.\n",
    "\n",
    "We can use any function to represent values as long as it returns a single scalar value for a state-action pair. For simplicity, we will be using a linear function. Function approximation methods can be used in both Sarsa and Q learning. In this homework, we will be implementing function approximation with Q learning.\n",
    "\n",
    "You can find a similar algorithm within the pseudocode at **chapter 9.3 in Reinforcement Learning Book (Sutton & Barto)**. These methods are called **semi-gradient** methods. The reason is that we do not take the gradient of the loss term(mean squared TD) with respect to both functions $Q(s,a)$ and $\\max_i Q(s', a_i)$.\n",
    "\n",
    "Please read the relevant section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11)** In tabular settings, whenever we update the value of a particular state action pair $(s, a)$, only a single value changes. What are your thoughts about the value changes in approximate settings after an update? How is it different than the tabular case? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework, we will be using the **CartPole** environment. It is a continuous state, discrete action environment. You can check it out from [Open AI gym](https://gym.openai.com/envs/CartPole-v0/) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12)** We will be using a linear function. However, we still need to have some features over raw observation. Complete the implementation of the wrapper below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "\n",
    "\n",
    "class FeaturedObservation(ObservationWrapper):\n",
    "    \"\"\" Create feature vectors that are randomly scattered in the observation\n",
    "    space. Instead of returning the raw state(observation) return features.\n",
    "    The feature matrix is a KxN matrix where K denotes the number of features while N\n",
    "    denotes raw state space size. Each row in the Feature matrix represents a feature\n",
    "    in the state space.\n",
    "    \n",
    "    F(S): R^{N} -> R^{K}  \n",
    "    \n",
    "    Returned vector is simply a dot product between each feature vector and the\n",
    "    observed state(observation).\n",
    "    \n",
    "    Arguments:\n",
    "        - env: gym environment\n",
    "        - n_features: Number of features\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_features):\n",
    "        super().__init__(env)\n",
    "        self.features = np.random.uniform(\n",
    "            -0.4, 0.4,\n",
    "            size=(n_features, env.observation_space.shape[0]))\n",
    "        _low = env.observation_space.low.min()\n",
    "        _high = env.observation_space.high.max()\n",
    "        _dtype = env.observation_space.dtype\n",
    "        self.observation_space  = Box(low=_low,\n",
    "                                      high=_high,\n",
    "                                      shape=(n_features,),\n",
    "                                      dtype=_dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\" Return features of the obervation using matrix multiplication.\n",
    "        \"\"\"\n",
    "        #  ______   _____   _        _\n",
    "        # |  ____| |_   _| | |      | |\n",
    "        # | |__      | |   | |      | |\n",
    "        # |  __|     | |   | |      | |\n",
    "        # | |       _| |_  | |____  | |____\n",
    "        # |_|      |_____| |______| |______|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 13)** In the ```rl_hw1/learning/approximate.py``` module, implement ```ApproximateAgent``` & ```ApproximateQAgent```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 14)** Run the approximate q learning agent by tuning the hyperparameters (```args```) given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rl_hw1.learning import ApproximateQAgent\n",
    "\n",
    "# Hyperparameters\n",
    "args = dict(\n",
    "    episodes = 800,\n",
    "    evaluate_period = 200,\n",
    "    n_features = 128,\n",
    "    gamma = 0.98,\n",
    "    alpha = 0.001,\n",
    "    init_eps = 0.9,\n",
    "    final_eps = 0.05,\n",
    "    eps_decay_rate = 0.99999,\n",
    "    seed = 12021,            # Current year: (10000 + 2021 = 12021) Holocene calendar\n",
    ")\n",
    "args = namedtuple('args', args.keys())(*args.values())\n",
    "\n",
    "# Initiate environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Seed\n",
    "env.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "# Wrap the environment\n",
    "env = FeaturedObservation(env, args.n_features)\n",
    "\n",
    "# Initialize agent\n",
    "appx_q_agent = ApproximateQAgent(\n",
    "    nobs = env.observation_space.shape[0],\n",
    "    nact = env.action_space.n\n",
    ")\n",
    "    \n",
    "appx_q_rewards, appx_q_losses = appx_q_agent.train(env, appx_q_agent.e_greedy_policy, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Approximate q learning training\")\n",
    "plt.xlabel(\"episodic reward\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.plot(appx_q_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate your agent. We expect to see at least **100** episodic reward in average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run this cell to visualize the agent\n",
    "# If you run evaluation with render from docker it may not work!!\n",
    "appx_q_agent.evaluate(env, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appx_q_agent_rewards = [appx_q_agent.evaluate(env, render=False) for i in range(100)]\n",
    "print(\"Average episodic reward: {:.4}, Deviation: {:.4}\".\n",
    "      format(np.mean(appx_q_agent_rewards), np.std(appx_q_agent_rewards)))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
