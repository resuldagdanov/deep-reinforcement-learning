{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "Here we will be studying Markov Decision Processes. But before that, let's remember Random Processes and Random Sequences. In Reinforcement Learning (RL) we always deal with discrete-time random  processes, which are formally named as random sequences.\n",
    "\n",
    "RL is fundamentally built on the top of Markov Decision Processes that satisfy the Markov property. But why? One of the crucial aspects of RL is that we deal with sequential problems in which the decisions are made sequentially over discrete time steps. But so far, we don't need to have Markov property. In the following implementations, you will see why assuming this property is important and makes everything simpler. Although not every problem fundamentally obeys the Markov property, either they can be converted into one or the assumption is sufficient to solve the decision problem.\n",
    "\n",
    "- **Question 1)**\n",
    "    Write down the Markov property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent acts in the environment geting transition interactions as $ S_{t-1}, A_{t-1}, R_t, S_{t}, A_{t}, R_{t+1} $ sequence where t is timesteps, $ S_{t} $ is a state at timestep ${t}$ where action $A_{t-1}$ is done getting reward $R_{t}$. If the state satisfies this property, then Markov property of the state is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RL, the objective is to maximize the possible gain, score or objective. Decisions are made at every timestep and a reward is observed after a transition from one state (where the decision is made) to another. Since the model we use is sequential, we are interested in maximizing the cumulative reward. We call this the credit assignment problem and it is one of the key process in RL. We don not initially know which actions (decisions) are crucial for obtaining a high cumulative reward. Solving this problem is very important for building a good decision making systems in RL.\n",
    "\n",
    "#### Modelling the World\n",
    "\n",
    "As stated before, we use MDPs to model environments. At every state, an agent may take an action and the environment transitions the agent into another state. At the transition, the agent gains an intermediate reward and this procedure repeats until the terminal state is reached in a finite horizon MDP. MDP and the agent produce a sequence of State $S$, Action $A$ and Reward $R$ at every timestep.\n",
    "\n",
    "$$ S_0, A_0, R_0, S_1, A_1, R_1, ... ,S_n, A_n, R_n $$\n",
    "\n",
    "This sequence/trajectory is known as a Markov Chain. Remember that in an MDP, every policy produces a Markov Chain.\n",
    "\n",
    "#### Evaluation of the decision\n",
    "\n",
    "We'd like to evaluate the quality of the decision, so that we can increase or decrease the likeliness of taking the decision. In other words, we need to assign the credit to the decision. One of the simple yet useful way to perform credit assignment is to use temporal difference error. We use __return__ to evaluate the value of a state and compare it with its expectation. We can define the __return__ of a state $S_{t_0}$ as the sum of all future rewards given that we start at state $S_{t_0}$ and follow a particular policy $\\pi$.  \n",
    "\n",
    "$$ G^\\pi(S_{t_0}) = \\sum_{t = t_0}^{\\infty} \\gamma^{t - t_0} R_t$$\n",
    "\n",
    "- **Question 2)**\n",
    "    Write down the possible reasons for using gamma as a discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) limits the termination length of the episode by the discounting factor.\n",
    "2) given less importance to the future upcoming rewards in the sense of total expected return.\n",
    "3) when $\\gamma$ is close to $1$, we are give similar amount of importance to the reward at every step in a Markov Chain; on the other hand, when $\\gamma$ is close to $0$, we are giving less and less importance to future upcoming rewards.\n",
    "4) $\\gamma$ gives control over how much credit to assign to future steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov property allows us to use Bellman equation to calculate the return G.\n",
    "\n",
    "- **Question 3)**\n",
    "    Write down the Bellman equation. How can you use it to calculate returns faster given that the Markov property is satisfied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Bellman equations we can assign memory for each state. We will call it the value of a state.\n",
    "\n",
    "$$ V^{\\pi}(s) = \\mathbb{E}[G_t | S_t = s]$$\n",
    "\n",
    "Remember that changing your decisions at a state will change the transition probabilities and that will affect returns as well. That is why, for each policy $\\pi$, we have different returns and values. Essentially, __each policy has a corresponding Value function__.\n",
    "\n",
    "Using the Bellman equation, we can calculate the value of every state. This allows us to optimize the policy we follow by simply choosing an action that will lead us to a state with the highest value. However, sometimes, the environment does not transition the agent into the state that we aim for. Therefore, we model the transition dynamics stochastically by $T(s'|s, a)$.\n",
    "\n",
    "- **Question 4)**\n",
    "    The value function is an expectation of the return $G$ starting from state $S$. Under which random variables that the expectation is taken over? State their density functions (such as $p(x | y)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use another expectation but this time for the decision. We will be calling it $Q$ function of a state and action pair.\n",
    "\n",
    "$$ Q^\\pi(s, a) = \\mathbb{E}[G | S_t=s, A_t=a]$$\n",
    "\n",
    "This makes the decision process easier as we can simply take the action with the highest Q value.\n",
    "\n",
    "- **Question 5)**\n",
    "    Both $Q^\\pi(s, a)$ function and $V^\\pi(s)$ function are expectations of the return. Write down the $Q^\\pi(s, a)$ function in terms of $V^\\pi(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Question 6)**\n",
    "    Let's define an initial state distribution $\\rho(s)$. Assume that you know the transition dynamics of the model and you want to choose from two policies. How can you compare the performance of the two policies $\\pi_0$ and $\\pi_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before, we can update a policy using the value functions (either $Q$ or $V$) if we know the transition dynamics and the reward function. A policy $\\pi$ is defined to be better than or equal to a policy $\\pi'$ if its expected return is greater than or equal to that of $\\pi'$ for all states.\n",
    "\n",
    "- **Question 7)**\n",
    "    The optimal value is denoted by $V^*$. Write down the optimal value in terms of $V^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Lets initialize an environment where we can practice with value functions, returns and policies. The first step is to build the environment. Luckily, we have a few environments in the repository(src/env). We will be using **MazeWorld** environment that is built with pycolab package.\n",
    "\n",
    "Mazeworld environment is a **gym** like environment where you need to call ```reset()``` to initiate the enviroment. Use ```step(action)``` to iterate one step with the given action. States in Mazeworld are the position of the player (x, y). There are 4 possible actions:\n",
    "- Up: 0\n",
    "- Down: 1\n",
    "- Right: 3\n",
    "- Left: 2\n",
    "\n",
    "In order to render the environment, you need to call ```init_render()``` to initiate renderer. We use ipycanvas to render the board.\n",
    "\n",
    "Note that the maximum time length is 200 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_hw1.env import MazeWorld\n",
    "import time\n",
    "\n",
    "worldmap = [\n",
    "     \"#######\",\n",
    "     \"#    @#\",\n",
    "     \"#     #\",\n",
    "     \"#     #\",\n",
    "     \"#     #\",\n",
    "     \"#P    #\",\n",
    "     \"#######\"]\n",
    "\n",
    "env = MazeWorld(worldmap=worldmap, cell_size=40)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a policy so that we can calculate returns and values.\n",
    "\n",
    "**Question 8)** First, run the cells below. Then, change the dumb policy so that it can reach the goal. You do not need to use sophisticated methods, just tweak the policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb_policy = lambda x, y: 0\n",
    "env.init_render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "step_count = 0\n",
    "while not done and step_count < 20:\n",
    "    action = dumb_policy(*state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    time.sleep(0.1)\n",
    "    step_count += 1\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a single example of the environments. They all have similar structure, ```step```, ```reset``` and ```render```."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
