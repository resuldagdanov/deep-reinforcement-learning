{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "The general organization of the homework is given below.\n",
    "\n",
    "- pg\n",
    "    - reinforce\n",
    "        - model\n",
    "        - cartpole\n",
    "    - a2c\n",
    "        - model\n",
    "        - vecenv\n",
    "        - box2d\n",
    "        - pong\n",
    "    - ppo\n",
    "        - model\n",
    "        - box2d\n",
    "        - walker\n",
    "    - common\n",
    "\n",
    "In this homework, you will implement REINFORCE, A2C, and PPO agents and run these agents in CartPole, LunarLander, Pong, and BipedalWalker environments.\n",
    "\n",
    "### Running\n",
    "\n",
    "Each experiment will be trained from scratch with 3 different seeds (except for Pong) to have a good understanding of the stochasticity involved in the training. You can run your experiments with command-line arguments from the jupyter notebook as shown below or using a bash script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T18:16:42.746345Z",
     "start_time": "2021-05-26T18:16:38.889016Z"
    }
   },
   "outputs": [],
   "source": [
    "!python pg/a2c/box2d.py --nenv 16 --log-dir log/a2c_lunar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You should obtain scores higher than below:\n",
    "- CartPole: 400\n",
    "- LunarLander: 200\n",
    "- BipedalWalker: 100\n",
    "- Pong: 10\n",
    "\n",
    "The default hyperparameters are not tuned but tested and they work well. However, hyperparemeters are sensitive to implementation details and hence you may need to tune them if you feel the need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the log file (named: progress.csv) will be saved in a temporary file if ```--log_dir``` CL argument is not given. You can use the csv file to obtain a Pandas dataframe object. Using the dataframe object, you can visualize the training with the given visualization script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T21:05:22.170741Z",
     "start_time": "2021-05-26T21:05:22.168527Z"
    }
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission should include logs and models folders.\n",
    "\n",
    "Example log folder:\n",
    "- log\n",
    "    - a2c_lunar\n",
    "        - 05-23-2021-00-01-02\n",
    "        - 05-23-2021-01-02-02\n",
    "        - 05-23-2021-02-03-02\n",
    "    - a2c_pong\n",
    "        - 05-23-2021-03-01-02\n",
    "    - ppo_lunar\n",
    "        - 05-23-2021-04-01-02\n",
    "        - 05-23-2021-05-02-02\n",
    "        - 05-23-2021-06-03-02\n",
    "    - ppo_walker\n",
    "        - 05-23-2021-07-01-02\n",
    "        - 05-23-2021-08-02-02\n",
    "        - 05-23-2021-09-03-02\n",
    "    - reinforce_cartpole\n",
    "        - 05-23-2021-10-01-02\n",
    "        - 05-23-2021-11-02-02\n",
    "        - 05-23-2021-12-03-02\n",
    "        \n",
    "As long as you use CL argument ```--log-dir``` with the values such as ```log/a2c_lunar```, the folder will be filled automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "When you are done with experiments, you can plot the statistics. As long as the number of rows in every log within the given directory matches ```Plotter``` draws the statistics of the runs. For example, if you have 3 runs for a certain experiment, ```Plotter``` draws a mean curve and a shaded region of the area between the $\\alpha$ and $1-\\alpha$ quantiles of the runs.\n",
    "\n",
    "Below is an example plotting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pg.visualize import Plotter\n",
    "\n",
    "log_dir = os.path.join(\"log\", \"a2c_lunar\")\n",
    "df_dict = {\n",
    "    \"lunar a2c\": [pd.read_csv(os.path.join(log_dir, folder, \"progress.csv\"))\n",
    "                for folder in os.listdir(log_dir)]\n",
    "}\n",
    "\n",
    "plotter = Plotter(df_dict)\n",
    "plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T18:17:47.746506Z",
     "start_time": "2021-05-26T18:17:47.744173Z"
    }
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforce\n",
    "\n",
    "The implementation of REINFORCE is pretty simple. There are two python scripts and we begin with filling the \"model.py\" script. Since you are familiar with REINFORCE, we can start implementing the ```learn``` method in the ```Reinforce``` class. It excepts ```args```, ```opt```, and ```env``` as parameters. We loop as many episodes as stated in the argument ```args.n_episodes``` and collect rollouts to update the policy parameters. Note that, if an episode does not terminate in ```args.max_episode_len``` many steps, we terminate it manually.\n",
    "\n",
    "We need to obtain a rollout that consists of a list of transitions. Each transition in the rollout needs to include the log probability of the selected (and performed) action and the intermediate reward. We expect from the ```policynet``` to return a Categorical distribution of the actions for a given state whenever the forward method is called. At each iteration in an episode, use the action distribution to sample action and step the environment with it. Store the log probability of the sampled action and the intermediate reward in a ```Transition``` object. The rollout is a list of ```Transition```s created during the episode. \n",
    "\n",
    "When we have the rollout, we can calculate gradients with respect to the policy parameters. Fill the missing part in ```accumulate_gradient```. Don't forget to call ```.backward()``` for each log probability to minimize the negative log-likelihood. Note that, we do not update the parameters in this method but only calculate the gradients.\n",
    "\n",
    "Now, fill the ```cartpole.py``` script. Fill the missing parts in the ```PolicyNet``` class so that it can return a categorical distribution for a given state.\n",
    "\n",
    "After the implementation is completed, you can run the experiments. Don't forget to tune the hyperparameters as they affect the performance of the training.\n",
    "\n",
    "### Experiments\n",
    "\n",
    "- Run REINFORCE in CartPole-v1 (Reach at least +400 mean reward for the last 20 episodes)\n",
    "\n",
    "\n",
    "By default, the writer logs the mean reward of the last 20 episodes. This can be changed by overwriting --log-window-length command-line argument.\n",
    "\n",
    "Plot the results. Also, please **keep** the ```progress.csv``` files as shown under the submission section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pg/reinforce/cartpole.py --log-dir log/reinforce_cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C Implementation\n",
    "\n",
    "A2C is a synchronized version of the popular A3C algorithm. The synchronization is done via a wrapper that runs multiple environments in parallel. We use the wrapper provided in ```pg/ac2/vecenv.py``` script. Unlike in REINFORCE algorithm, we do not collect rollouts until the termination. Instead, the rollouts in A2C consist of fixed length transitions in parallel.\n",
    "\n",
    "Although you do not need to fill ```pg/ac2/vecenv.py``` please take a look to understand the vectorization of the environments. \n",
    "\n",
    "Before starting, you may want to check ```learn``` method in the ```pg/ac2/model.py``` script to observe the overall structure of the algorithm.\n",
    "\n",
    "#### Model\n",
    "\n",
    "Fill the missing part in ```forward``` method in ```pg/ac2/model.py``` script. You may want to check the return value of the network at ```pg/ac2/box2d.py``` to know what to expect when you call ```self.network(.)``` in the ```forward``` method. We want to sample actions and calculate the log probabilities of these actions as well as the entropy of the action distribution to use them later in the parameter update.\n",
    "\n",
    "Next, you need to fill ```collect_rollout``` method in the ```pg/ac2/model.py``` script. This is the method that collects ```args.n-step``` many transitions in parallel to make a rollout. Note that, you need to calculate the value of the last state (the one that is returned by the ```step``` function at the end of the rollout) so that we can calculate the target value later. Combining the list of transitions and the value of the last state you can form a ```Rollout``` object. We also return the last state and the last gru hidden state for future calls.\n",
    "\n",
    "We continue filling with ```calculate_gae``` method in the ```pg/ac2/model.py``` script. You need to read GAE paper before implementing this part. This method returns a list of advantages and a list of returns (the capital letter G in the book notation). We will use the advantages to calculate the policy loss and use returns to calculate value loss.\n",
    "\n",
    "Now we have a rollout and a list of advantages and returns, we can calculate a loss and update the parameters. Fill ```parameter_update``` method in the ```pg/ac2/model.py``` script. You can use ```rollout_data_loader``` method to obtain flatten tensors.\n",
    "\n",
    "Finally, fill the ```evaluate``` method that runs the policy for multiple episodes to obtain an average score. We use this method to measure the performance of a trained agent. Now that we filled all of the missing parts, we can observe how they come together in the ```learn``` method at ```pg/ac2/model.py```.\n",
    "\n",
    "#### Box2d\n",
    "\n",
    "Now we need to create a neural network that represents the policy and the value functions. Unlike before, we will use a recurrent layer (GRU layer). That is why we have additional tensors like ```gru_hx``` in the ```forward``` and ```collect_rollout``` methods. We assume a familiarity with the recurrent networks for this part.\n",
    "\n",
    "Start filling the ```__init__``` method. You can use separate networks to represent the policy and the value functions or a shared feature network and two separate head layers. Next, fill the ```forward``` method. Remember to return policy logits (no nonlinearity so that you can use them in the ```forward``` method in the ```pg/ac2/model.py``` script to create a Categorical distribution), value (no nonlinearity), and the hidden vector for the GRU layer. The Categorical distribution will be created in the ```A2C``` class and not within the ```network```.\n",
    "\n",
    "#### Pong\n",
    "\n",
    "Pong is a visual domain, so you may want to use convolutional layers (not mandatory). Other than that, there is only a handful of differences between the Pong network and the box2d network. We use simple environment wrappers that are designed specifically for the Pong environment. You don't have to use a GPU in this experiment as it trains quite fast given the implementation is correct.\n",
    "\n",
    "\n",
    "#### Experiments\n",
    "When you complete all the implementations mentioned above, you can start experimenting (and debugging) with  LunarLander and Pong environments.\n",
    "\n",
    "We will run two experiments.\n",
    "\n",
    "- Run A2C in LunarLander-v2 (Reach at least +200 mean reward for the last 20 episodes)\n",
    "- Run A2C in Pong (Reach at least 10 mean reward for the last 20 episodes) (a single seed is enough)\n",
    "\n",
    "Plot these results (2 Plots). Also **keep** the ```.csv``` files and the models (you can leave a Google Drive link for the model files if you can not submit them through Ninova)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pg/a2c/box2d.py --log-dir ./log/a2c_lunar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T19:30:35.794695Z",
     "start_time": "2021-05-26T19:30:35.792457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pg/a2c/pong.py --log-dir ./log/a2c_pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T19:30:35.969247Z",
     "start_time": "2021-05-26T19:30:35.967449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Pong 5 times\n",
    "import torch\n",
    "\n",
    "from pg.a2c.pong import make_env, GruNet\n",
    "from pg.a2c.model import A2C\n",
    "\n",
    "\n",
    "model_data = torch.load(\"models/pong.b\")\n",
    "env = make_env()\n",
    "in_size = env.observation_space.shape[0]\n",
    "out_size = env.action_space.n\n",
    "\n",
    "network = GruNet(in_size, out_size, model_data[\"args\"][\"hidden_size\"])\n",
    "model = A2C(network, None, None, None)\n",
    "model.load_state_dict(model_data[\"state_dict\"])\n",
    "model.evaluate(make_env, n_episodes=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Implementation\n",
    "\n",
    "PPO is very similar to A2C in terms of implementation steps. Most of the structure is the same and you can follow the same order as you did in the A2C part. In PPO experiments, we will not use recurrent architecture as that makes the implementation a bit challenging. There are two experiments with PPO, LunarLander, and BipedalWalker. Note that, BipedalWalker is a continuous action space environment.\n",
    "\n",
    "#### Differences\n",
    "\n",
    "- ```parameter_update``` method updates the parameters multiple times, one per mini-batch.\n",
    "- Unlike in A2C, we have ```forward_given_actions``` method that is used within ```parameter_update``` method to calculate the log_probabilities, values, and entropies over multiple passes. Since, after every mini-batch, the parameters are updated we can not use the same log_probabilities and need to recalculate them.\n",
    "- ```rollout_data_loader``` method needs to yield mini-batches of rollout data as opposed to full rollout data.\n",
    "- You need to fill ```linear_annealing``` method that is used for scheduling ```clip_range``` parameter.\n",
    "\n",
    "#### Box2d\n",
    "\n",
    "Similar to A2C.\n",
    "\n",
    "#### BipedalWalker\n",
    "\n",
    "This is a continuous action space environment. Use normal distribution to represent action distributions. \n",
    "\n",
    "#### Experiments\n",
    "\n",
    "We will run two experiments each containing 3 runs as mentioned previously.\n",
    "\n",
    "- Run PPO in LunarLander-v2 (Reach at least +200 mean reward for the last 20 episodes)\n",
    "- Run PPO in BipedalWalker-v2 (Reach at least +100 mean reward for the last 20 episodes)\n",
    "\n",
    "> Notice: Recent gym versions may require BipedalWalker-v3 instaed of BipedalWalker-v2. Please change it accordingly if necessary. \n",
    "\n",
    "Plot these results (2 Plots). Also **keep** the ```.csv``` files and the models (you can leave a Google Drive link for the model files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pg/ppo/box2d.py --log-dir ./log/ppo_lunar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pg/ppo/walker.py --log-dir ./log/ppo_walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Walker agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "Now that you completed the implementations you can compare the training performance of A2C and PPO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pg.visualize import Plotter\n",
    "\n",
    "a2c_log_dir = os.path.join(\"log\", \"a2c_lunar\")\n",
    "ppo_log_dir = os.path.join(\"log\", \"ppo_lunar\")\n",
    "df_dict = {\n",
    "    \"a2c Lunarlander\": [pd.read_csv(os.path.join(a2c_log_dir, folder, \"progress.csv\"))\n",
    "                for folder in os.listdir(a2c_log_dir)],\n",
    "    \"ppo Lunarlander\": [pd.read_csv(os.path.join(ppo_log_dir, folder, \"progress.csv\"))\n",
    "                for folder in os.listdir(ppo_log_dir)],\n",
    "}\n",
    "\n",
    "plotter = Plotter(df_dict)\n",
    "plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your comments (Bonus + 5)\n",
    "\n",
    "> Explain the score comparison you observe on Lunar Lander environment.\n",
    "\n",
    "> Explain the advantages and disadvantages of the methods you implemented within this homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.7.11 ('drl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "8048370e5f4472fc8019254cb1a7d72a501871a1608696a966bb718ea10e97a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
